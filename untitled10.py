# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ayDWZZ9DSzsV1KmEKwW3uPpLHjl_R6c

Q1
"""

import numpy as np

def softmax(x):
    """
    Computes the softmax of a matrix/vector along the last dimension for numerical stability.

    Parameters:
        x (np.ndarray): The input array of unnormalized scores.

    Returns:
        np.ndarray: The array with softmax applied (Attention Weights).
    """
    # Subtract the max for numerical stability (to prevent overflow)
    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    # Divide by the sum to get probabilities
    return e_x / np.sum(e_x, axis=-1, keepdims=True)

def scaled_dot_product_attention(Q, K, V):
    """
    Computes the scaled dot-product attention according to the formula:
    Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V

    Parameters:
        Q (np.ndarray): Query matrix (batch_size, sequence_length, d_k).
        K (np.ndarray): Key matrix (batch_size, sequence_length, d_k).
        V (np.ndarray): Value matrix (batch_size, sequence_length, d_v).

    Returns:
        tuple: (context_vector (np.ndarray), attention_weights (np.ndarray))
    """
    # 1. Determine d_k (dimension of keys/queries)
    d_k = Q.shape[-1]

    # 2. Compute the unscaled alignment scores: Q * K^T
    # np.transpose(K, axes=(0, 2, 1)) transposes the last two dimensions (d_k and sequence_length)
    scores = np.matmul(Q, np.transpose(K, axes=(0, 2, 1)))

    # 3. Scale the scores by 1/sqrt(d_k)
    scale_factor = np.sqrt(d_k)
    scaled_scores = scores / scale_factor

    # 4. Normalize scores using softmax to get Attention Weights
    attention_weights = softmax(scaled_scores)

    # 5. Compute the context vector: Attention Weights * V
    context_vector = np.matmul(attention_weights, V)

    return context_vector, attention_weights

# --- Example Usage and Verification ---

# Create dummy matrices (Batch size=2, Sequence length=4, d_k=3, d_v=5)
np.random.seed(42)
B, S, Dk, Dv = 2, 4, 3, 5
Q_test = np.random.rand(B, S, Dk)
K_test = np.random.rand(B, S, Dk)
V_test = np.random.rand(B, S, Dv)

# Compute attention
C, W = scaled_dot_product_attention(Q_test, K_test, V_test)

print(f"--- Q1: Scaled Dot-Product Attention Verification ---")
print(f"Q Shape: {Q_test.shape}")
print(f"V Shape: {V_test.shape}")
print("-" * 40)

print(f"Attention Weights Shape (Batch, Seq_Q, Seq_K): {W.shape}")
# Verify softmax normalization: sum of each row (dim=-1) should be 1.0
print(f"Sum of Weights for first query in first batch: {np.sum(W[0, 0, :]):.6f}")

print(f"\nContext Vector Shape (Batch, Seq, D_v): {C.shape}")
print("-" * 40)

"""Q2"""

import torch
import torch.nn as nn

# Sub-task a) Initialize dimensions
D_MODEL = 128
NUM_HEADS = 8
D_FF = 512  # Hidden dimension for the Feed-Forward Network
DROPOUT = 0.1

class TransformerEncoderBlock(nn.Module):
    """
    A simplified Transformer Encoder Block implementation in PyTorch.
    This block includes Multi-Head Attention, a Feed-Forward Network (FFN),
    and the necessary Add & Norm layers (residual connections and Layer Normalization).

    The structure follows the canonical Transformer architecture:
    Input -> MHA Sub-Layer -> Add & Norm 1 -> FFN Sub-Layer -> Add & Norm 2 -> Output
    """
    def __init__(self, d_model=D_MODEL, num_heads=NUM_HEADS, d_ff=D_FF, dropout=DROPOUT):
        """
        Initializes the components of the Encoder Block.
        """
        super().__init__()

        # 1. Multi-Head Attention (MHA) Layer
        # batch_first=True makes the input shape (Batch, Sequence, Feature)
        self.attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        # 2. Feed-Forward Network (FFN)
        # (2 linear layers with ReLU activation)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff), # First linear layer: d_model -> d_ff
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)  # Second linear layer: d_ff -> d_model
        )

        # 3. Add & Norm Layers (Layer Normalization)
        # Sub-task b) Add residual connections and layer normalization.
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        """
        Defines the forward pass through the Encoder Block.
        """

        # --- 1. Multi-Head Attention Sub-Layer ---
        # Self-Attention (Q=K=V=x)
        # Note: The second returned value is attention weights, which we discard here.
        attn_output, _ = self.attn(query=x, key=x, value=x)

        # Add & Norm 1: Residual connection + Layer Norm
        # The residual connection is the addition of the input 'x' and the MHA output.
        norm1_output = self.norm1(x + self.dropout1(attn_output))

        # --- 2. Feed-Forward Sub-Layer ---
        ffn_output = self.ffn(norm1_output)

        # Add & Norm 2: Residual connection + Layer Norm
        # The residual connection is the addition of the norm1_output and the FFN output.
        final_output = self.norm2(norm1_output + self.dropout2(ffn_output))

        return final_output

# --- Example Usage and Verification ---

# Sub-task c) Verify the output shape for a batch of 32 sentences, each with 10 tokens.
B, S, D = 32, 10, D_MODEL  # Batch=32, Sequence=10, d_model=128

# Create an instance of the encoder block
encoder_block = TransformerEncoderBlock()

# Create a dummy input tensor
dummy_input = torch.randn(B, S, D) # Shape (32, 10, 128)

# Pass the input through the block
output = encoder_block(dummy_input)

print(f"--- Q2: Transformer Encoder Block Verification (PyTorch) ---")
print(f"d_model={D_MODEL}, num_heads={NUM_HEADS}")
print(f"Input Tensor Shape:  {dummy_input.shape}")
print(f"Actual Output Shape: {output.shape}")

# Verification check
expected_shape = (B, S, D)
if output.shape == expected_shape:
    print(f"\nVerification successful: Output shape {output.shape} matches expected shape {expected_shape}.")
else:
    print(f"\nVerification FAILED: Output shape {output.shape} does not match expected shape {expected_shape}.")

print("-" * 40)